version: '3.8'

services:
  # PostgreSQL Database
  postgres:
    image: postgres:15-alpine
    container_name: ai_seller-db
    environment:
      POSTGRES_DB: selling
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: Ma3172003
      POSTGRES_HOST_AUTH_METHOD: trust
      # Performance tuning
      POSTGRES_SHARED_BUFFERS: 256MB
      POSTGRES_EFFECTIVE_CACHE_SIZE: 1GB
      POSTGRES_MAX_CONNECTIONS: 100
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init-db.sql:/docker-entrypoint-initdb.d/init-db.sql:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres -d selling"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - ai-agent-network
    restart: unless-stopped
    # Resource limits
    deploy:
      resources:
        limits:
          memory: 512M
        reservations:
          memory: 256M

  # Redis (for caching and sessions)
  redis:
    image: redis:7-alpine
    container_name: ai_seller-redis
    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5
      start_period: 10s
    networks:
      - ai-agent-network
    restart: unless-stopped
    # Resource limits
    deploy:
      resources:
        limits:
          memory: 256M
        reservations:
          memory: 128M

  # Ollama Service (for LLM inference)
  ollama:
    image: ollama/ollama:latest
    container_name: ai_seller-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
      - OLLAMA_NUM_PARALLEL=2
      - OLLAMA_MAX_LOADED_MODELS=1
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - ai-agent-network
    restart: unless-stopped
    # GPU support for Ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    # Pull required model on startup
    entrypoint: >
      sh -c "
        ollama serve &
        sleep 30 &&
        ollama pull gemma2:2b &&
        wait
      "

  # Main AI Agent Seller Backend Application
  backend:
    build:
      context: .
      dockerfile: docker/Dockerfile
      args:
        - BUILDKIT_INLINE_CACHE=1
    image: ai_seller-backend:latest
    container_name: ai_seller-backend
    ports:
      - "8000:8000"
    environment:
      # Database
      POSTGRES_SERVER: postgres
      POSTGRES_PORT: 5432
      POSTGRES_DB: selling
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: Ma3172003
      DATABASE_URL: postgresql+psycopg2://postgres:Ma3172003@postgres:5432/selling
      
      # Redis
      REDIS_HOST: redis
      REDIS_PORT: 6379
      REDIS_URL: redis://redis:6379/0
      
      # Server
      SERVER_HOST: 0.0.0.0
      SERVER_PORT: 8000
      
      # Ollama
      OLLAMA_ENDPOINT: http://ollama:11434/api/generate
      OLLAMA_BASE_URL: http://ollama:11434
      
      # CUDA/GPU
      CUDA_VISIBLE_DEVICES: 0
      USE_GPU: "true"
      TORCH_CUDA_ARCH_LIST: "8.6"  # Adjust based on your GPU architecture
      
      # Application settings
      LOG_LEVEL: INFO
      ENVIRONMENT: production
      DEBUG: "false"
      PRELOAD_MODELS: "true"
      
      # Model paths - updated to match your assets structure
      WHISPER_MODEL_PATH: /app/assets/models/whisper
      TTS_MODEL_PATH: /app/assets/models/tts
      YOLO_MODEL_PATH: /app/assets/models/yolov11x-person-mot20val-crowdhuman.pt
      REID_MODEL_PATH: /app/assets/models/fine_tuned_model_epoch_32.pth
      TTS_SPEAKER_AUDIO: /app/assets/audio/_7JpEjF2Vyk.wav
      
      # Security
      SECRET_KEY: your-secret-key-change-in-production
      ACCESS_TOKEN_EXPIRE_MINUTES: 30
      
      # Performance
      WORKERS: 1
      MAX_REQUESTS: 1000
      MAX_REQUESTS_JITTER: 50
      
    volumes:
      - ./uploads:/app/uploads:rw
      - ./logs:/app/logs:rw  
      - ./assets:/app/assets:rw
      - ./models:/app/models:rw
      # Mount specific model files from root directory
      - ./fine_tuned_model_epoch_32.pth:/app/assets/models/fine_tuned_model_epoch_32.pth:ro
      - ./yolov11x-person-mot20val-crowdhuman.pt:/app/assets/models/yolov11x-person-mot20val-crowdhuman.pt:ro
      - ./_7JpEjF2Vyk.wav:/app/assets/audio/_7JpEjF2Vyk.wav:ro
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      ollama:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/status/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    networks:
      - ai-agent-network
    restart: unless-stopped
    # Enable GPU access for ML models
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 4G
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
  ollama_data:
    driver: local

networks:
  ai-agent-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16