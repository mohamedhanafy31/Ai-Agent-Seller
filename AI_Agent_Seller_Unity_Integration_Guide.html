
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Agent Seller Backend - Unity Integration Guide</title>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap');
        
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.6;
            color: #2d3748;
            background: #ffffff;
            font-size: 14px;
            max-width: 210mm;
            margin: 0 auto;
            padding: 20px;
        }
        
        /* Headers */
        h1 {
            font-size: 2.5rem;
            font-weight: 700;
            color: #1a202c;
            margin-bottom: 1rem;
            text-align: center;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }
        
        h2 {
            font-size: 1.8rem;
            font-weight: 600;
            color: #2d3748;
            margin: 2rem 0 1rem 0;
            padding-bottom: 0.5rem;
            border-bottom: 2px solid #e2e8f0;
        }
        
        h3 {
            font-size: 1.4rem;
            font-weight: 600;
            color: #4a5568;
            margin: 1.5rem 0 0.8rem 0;
            padding: 12px 16px;
            background: linear-gradient(135deg, #f7fafc 0%, #edf2f7 100%);
            border-left: 4px solid #667eea;
            border-radius: 0 8px 8px 0;
        }
        
        h4 {
            font-size: 1.1rem;
            font-weight: 600;
            color: #2d3748;
            margin: 1rem 0 0.5rem 0;
        }
        
        /* Paragraphs and text */
        p {
            margin-bottom: 1rem;
            text-align: justify;
        }
        
        /* Lists */
        ul, ol {
            margin: 1rem 0;
            padding-left: 2rem;
        }
        
        li {
            margin-bottom: 0.5rem;
        }
        
        /* Code blocks */
        pre {
            background: #f7fafc;
            border: 1px solid #e2e8f0;
            border-radius: 8px;
            padding: 1rem;
            margin: 1rem 0;
            overflow-x: auto;
            font-family: 'JetBrains Mono', 'Fira Code', monospace;
            font-size: 0.85rem;
            line-height: 1.4;
        }
        
        code {
            font-family: 'JetBrains Mono', 'Fira Code', monospace;
            background: #f7fafc;
            padding: 2px 6px;
            border-radius: 4px;
            font-size: 0.85rem;
            color: #e53e3e;
        }
        
        pre code {
            background: none;
            padding: 0;
            color: #2d3748;
        }
        
        /* Special divs with gradients */
        div[style*="background: linear-gradient"] {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%) !important;
            color: white !important;
            padding: 20px !important;
            border-radius: 10px !important;
            margin: 10px 0 !important;
        }
        
        div[style*="background: linear-gradient"] strong {
            color: white !important;
        }
        
        /* Center alignment */
        div[align="center"] {
            text-align: center;
            margin-bottom: 2rem;
        }
        
        div[align="center"] h2 {
            font-size: 1.5rem;
            color: #667eea;
            margin-bottom: 0.5rem;
            border: none;
        }
        
        div[align="center"] em {
            font-size: 1.1rem;
            color: #718096;
            font-style: italic;
        }
        
        /* Details/Summary */
        details {
            background: #f7fafc;
            border: 1px solid #e2e8f0;
            border-radius: 8px;
            padding: 1rem;
            margin: 1rem 0;
        }
        
        summary {
            font-weight: 600;
            cursor: pointer;
            padding: 0.5rem 0;
            color: #4a5568;
        }
        
        summary:hover {
            color: #667eea;
        }
        
        /* Horizontal rules */
        hr {
            border: none;
            height: 2px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            margin: 2rem 0;
        }
        
        /* Print styles */
        @media print {
            body {
                font-size: 12px;
                padding: 15px;
            }
            
            h1 {
                font-size: 2rem;
            }
            
            h2 {
                font-size: 1.5rem;
            }
            
            h3 {
                font-size: 1.2rem;
            }
            
            pre {
                font-size: 0.8rem;
            }
        }
    </style>
</head>
<body>
    <div align="center">

<h1>ğŸ¤– AI Agent Seller Backend</h1>
<h2>Real-Time Customer Interaction Flow</h2>

<em>Comprehensive Unity Integration Guide</em>

---

</div>

<h2>ğŸ¯ <strong>Executive Summary</strong></h2>

<p>This documentation provides a complete guide for implementing a real-time, AI-powered customer interaction system using Arabic speech recognition, intelligent conversation, and personalized responses. The system combines computer vision, natural language processing, and speech synthesis to create an immersive retail experience.</p>

<h3><strong>ğŸ”¥ Key Features</strong></h3>
<ul><li>ğŸ¥ <strong>Real-time Person Detection</strong> with YOLOv11x + BoostTrack</li>
<li>ğŸ¤ <strong>Live Arabic Speech Recognition</strong> with Whisper Large V3 Turbo</li>
<li>ğŸ§  <strong>Context-Aware AI Responses</strong> with Ollama Gemma 2</li>
<li>ğŸ‘ï¸ <strong>Person Status Analysis</strong> with VLM Gemma3</li>
<li>ğŸ”Š <strong>Real-time Arabic Text-to-Speech</strong> with XTTS-v2</li>
<li>âš¡ <strong>End-to-End Latency</strong> < 7 seconds from detection to response</li>

</ul>---

<h2>ğŸŒŠ <strong>Interaction Flow Pipeline</strong></h2>

<pre><code class="mermaid">graph TD
A[ğŸ“¹ Camera Stream] --> B[ğŸ¯ Person Detection]
B --> C[ğŸµ Arabic Greeting<br/>"Ø§Ù‡Ù„Ø§ Ø¨ÙŠÙƒ ØªØ¹Ø§Ù„ÙŠ"]
C --> D[ğŸ¤ Real-time STT<br/>Customer Speech]
D --> E[ğŸ‘ï¸ VLM Analysis<br/>Person Status]
E --> F[ğŸ§  Context-Aware LLM<br/>Intelligent Response]
F --> G[ğŸ”Š Real-time TTS<br/>Audio Response]
G --> H[ğŸ”„ Ready for Next Customer]

style A fill:#e1f5fe
style B fill:#f3e5f5
style C fill:#e8f5e8
style D fill:#fff3e0
style E fill:#fce4ec
style F fill:#e0f2f1
style G fill:#f1f8e9
style H fill:#e8eaf6</code></pre>

---

<h2>ğŸ“‹ <strong>Implementation Guide</strong></h2>

<h3>ğŸ¯ <strong>Step 1: Person Detection & Tracking</strong></h3>

<div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 20px; border-radius: 10px; margin: 10px 0;">

<strong>ğŸ”— Endpoint:</strong> <code>POST /api/v1/tracking/upload</code> + <code>POST /api/v1/tracking/process/{session_id}</code>

<strong>ğŸ¬ Process Flow:</strong>
<ul><li>ğŸ“¹ Camera stream is captured and uploaded as video</li>
<li>ğŸ” System detects person in the video using YOLOv11x</li>
<li>âœ… When person is detected â†’ triggers Step 2 (greeting)</li>

</ul></div>

<h4>ğŸ“¡ <strong>API Usage</strong></h4>

<details>
<summary><b>ğŸ”§ HTTP Request</b></summary>

<pre><code class="http">POST /api/v1/tracking/upload
Content-Type: multipart/form-data

file: [video_stream.mp4]
confidence_threshold: 0.5</code></pre>

<strong>ğŸ“¤ Response:</strong>
<pre><code class="json">{
"session_id": "track_session_abc123",
"status": "uploaded"
}</code></pre>

</details>

<h4>ğŸ§ª <strong>Postman Testing Example</strong></h4>

<details>
<summary><b>ğŸ“® Step-by-Step Postman Setup</b></summary>

<strong>ğŸ”§ Upload Video for Tracking:</strong>
<ul><li><strong>Method:</strong> <code>POST</code></li>
<li><strong>URL:</strong> <code>http://localhost:8000/api/v1/tracking/upload</code></li>
<li><strong>Headers:</strong> </li>
</ul>- <code>Content-Type</code>: <code>multipart/form-data</code> (auto-set)
<ul><li><strong>Body â†’ form-data:</strong></li>
</ul>- Key: <code>file</code> | Type: <code>File</code> | Value: Upload a <code>.mp4</code> video file
- Key: <code>confidence_threshold</code> | Type: <code>Text</code> | Value: <code>0.5</code>
- Key: <code>max_persons</code> | Type: <code>Text</code> | Value: <code>10</code>

<strong>ğŸ“¤ Expected Response:</strong>
<pre><code class="json">{
"session_id": "track_abc123",
"status": "uploaded",
"filename": "video.mp4",
"file_size": 2048576
}</code></pre>

<strong>ğŸ”§ Process Video Tracking:</strong>
<ul><li><strong>Method:</strong> <code>POST</code></li>
<li><strong>URL:</strong> <code>http://localhost:8000/api/v1/tracking/process/{session_id}</code></li>
</ul>- Replace <code>{session_id}</code> with the session_id from upload response
<ul><li><strong>Body â†’ form-data:</strong></li>
</ul>- Key: <code>confidence_threshold</code> | Type: <code>Text</code> | Value: <code>0.25</code>
- Key: <code>max_tracks</code> | Type: <code>Text</code> | Value: <code>100</code>

<strong>ğŸ“¤ Expected Response:</strong>
<pre><code class="json">{
"session_id": "track_abc123",
"status": "completed",
"persons_detected": 2,
"processing_time": 5.2,
"tracks": [
{
"track_id": 1,
"person_id": "person_001",
"confidence": 0.89,
"frames_tracked": 120
}
]
}</code></pre>

</details>

<h4>ğŸ® <strong>Unity Implementation</strong></h4>
<pre><code class="csharp">using UnityEngine;
using UnityEngine.Networking;
using System.Collections;

public class PersonTracker : MonoBehaviour
{
public string serverUrl = "http://localhost:8000";
public Camera trackingCamera;

public IEnumerator DetectPerson()
{
// Step 1: Upload video for tracking
byte[] videoData = CaptureVideoFromCamera();

WWWForm form = new WWWForm();
form.AddBinaryData("file", videoData, "camera_feed.mp4", "video/mp4");
form.AddField("confidence_threshold", "0.5");

using (UnityWebRequest request = UnityWebRequest.Post($"{serverUrl}/api/v1/tracking/upload", form))
{
yield return request.SendWebRequest();

if (request.result == UnityWebRequest.Result.Success)
{
var response = JsonUtility.FromJson<TrackingUploadResponse>(request.downloadHandler.text);
string sessionId = response.session_id;

// Step 2: Process the uploaded video
yield return StartCoroutine(ProcessTracking(sessionId));
}
}
}

private IEnumerator ProcessTracking(string sessionId)
{
using (UnityWebRequest request = UnityWebRequest.Post($"{serverUrl}/api/v1/tracking/process/{sessionId}", new WWWForm()))
{
yield return request.SendWebRequest();

if (request.result == UnityWebRequest.Result.Success)
{
var result = JsonUtility.FromJson<TrackingResult>(request.downloadHandler.text);

if (result.persons_detected > 0)
{
Debug.Log("Person detected! Starting greeting...");
GetComponent<TTSManager>().PlayGreeting();
}
}
}
}

private byte[] CaptureVideoFromCamera()
{
// Implement video capture from Unity camera
return System.IO.File.ReadAllBytes("sample_video.mp4");
}
}

[System.Serializable]
public class TrackingUploadResponse
{
public string session_id;
public string status;
}

[System.Serializable]
public class TrackingResult
{
public int persons_detected;
public string status;
}</code></pre>

---

<h3>ğŸµ <strong>Step 2: Generate Greeting Audio "Ø§Ù‡Ù„Ø§ Ø¨ÙŠÙƒ ØªØ¹Ø§Ù„ÙŠ"</strong></h3>

<div style="background: linear-gradient(135deg, #48bb78 0%, #38a169 100%); padding: 20px; border-radius: 10px; margin: 10px 0;">

<strong>ğŸ”— Endpoint:</strong> <code>WebSocket /api/v1/tts/stream</code> (Real-time)

<strong>ğŸ¬ Process Flow:</strong>
<ul><li>ğŸ¯ Person detected â†’ immediately generate greeting audio</li>
<li>ğŸ“¤ Send Arabic text "Ø§Ù‡Ù„Ø§ Ø¨ÙŠÙƒ ØªØ¹Ø§Ù„ÙŠ" to TTS</li>
<li>ğŸµ Receive audio chunks and play in real-time</li>
<li>â–¶ï¸ When greeting finishes â†’ triggers Step 3 (listening)</li>

</ul></div>

<h4>ğŸ“¡ <strong>API Usage</strong></h4>

<details>
<summary><b>ğŸ”§ WebSocket Request</b></summary>

<pre><code class="json">{
"text": "Ø§Ù‡Ù„Ø§ Ø¨ÙŠÙƒ ØªØ¹Ø§Ù„ÙŠ",
"language": "ar",
"chunk_size": 1024
}</code></pre>

<strong>ğŸ“¤ Response (Streaming):</strong>
<pre><code class="json">{
"type": "audio_chunk",
"data": "base64_audio_data",
"chunk_index": 1
}</code></pre>

</details>

<h4>ğŸ§ª <strong>Postman Testing Example</strong></h4>

<details>
<summary><b>ğŸ“® TTS WebSocket Testing (Advanced)</b></summary>

<strong>âš ï¸ Note:</strong> Postman WebSocket support is limited. For full testing, use a WebSocket client or the Unity implementation.

<strong>ğŸ”§ Alternative: Test TTS Batch Endpoint:</strong>
<ul><li><strong>Method:</strong> <code>POST</code></li>
<li><strong>URL:</strong> <code>http://localhost:8000/api/v1/tts/synthesize</code></li>
<li><strong>Headers:</strong></li>
</ul>- <code>Content-Type</code>: <code>application/json</code>
<ul><li><strong>Body â†’ raw (JSON):</strong></li>
</ul><pre><code class="json">{
"text": "Ø§Ù‡Ù„Ø§ Ø¨ÙŠÙƒ ØªØ¹Ø§Ù„ÙŠ",
"language": "ar",
"speed": 1.0,
"voice": "default"
}</code></pre>

<strong>ğŸ“¤ Expected Response:</strong>
<pre><code class="json">{
"audio_url": "/audio/output_12345.wav",
"duration": 2.8,
"sample_rate": 22050,
"processing_time": 0.8
}</code></pre>

<strong>ğŸµ Test Audio Playback:</strong>
<ul><li>Copy the <code>audio_url</code> from response</li>
<li>Open: <code>http://localhost:8000/audio/output_12345.wav</code> in browser</li>
<li>Should play Arabic greeting audio</li>

</ul><strong>ğŸ”§ WebSocket Testing (Using WebSocket Client):</strong>
<ul><li><strong>URL:</strong> <code>ws://localhost:8000/api/v1/tts/stream</code></li>
<li><strong>Send Message:</strong></li>
</ul><pre><code class="json">{
"text": "Ø§Ù‡Ù„Ø§ Ø¨ÙŠÙƒ ØªØ¹Ø§Ù„ÙŠ",
"language": "ar",
"chunk_size": 1024
}</code></pre>
<ul><li><strong>Receive:</strong> Base64 audio chunks in real-time</li>

</ul></details>

<h4>ğŸ® <strong>Unity Implementation</strong></h4>
<pre><code class="csharp">using UnityEngine;
using NativeWebSocket;
using System.Threading.Tasks;

public class TTSManager : MonoBehaviour
{
public string serverUrl = "http://localhost:8000";
public AudioSource audioSource;

private WebSocket ttsWebSocket;

async void Start()
{
string wsUrl = serverUrl.Replace("http://", "ws://") + "/api/v1/tts/stream";
ttsWebSocket = new WebSocket(wsUrl);

ttsWebSocket.OnOpen += () => Debug.Log("TTS WebSocket connected");
ttsWebSocket.OnMessage += HandleTTSMessage;

await ttsWebSocket.Connect();
}

public async void PlayGreeting()
{
if (ttsWebSocket.State == WebSocketState.Open)
{
var request = new TTSRequest
{
text = "Ø§Ù‡Ù„Ø§ Ø¨ÙŠÙƒ ØªØ¹Ø§Ù„ÙŠ",
language = "ar",
chunk_size = 1024
};

string json = JsonUtility.ToJson(request);
await ttsWebSocket.SendText(json);
}
}

private void HandleTTSMessage(byte[] bytes)
{
string message = System.Text.Encoding.UTF8.GetString(bytes);
var response = JsonUtility.FromJson<TTSResponse>(message);

switch (response.type)
{
case "audio_chunk":
PlayAudioChunk(response.data);
break;
case "complete":
Debug.Log("Greeting complete - starting to listen");
GetComponent<STTManager>().StartListening();
break;
}
}

private void PlayAudioChunk(string base64Audio)
{
byte[] audioData = System.Convert.FromBase64String(base64Audio);
// Convert bytes to AudioClip and play
AudioClip clip = CreateAudioClipFromBytes(audioData);
audioSource.clip = clip;
audioSource.Play();
}

private AudioClip CreateAudioClipFromBytes(byte[] audioData)
{
// Simplified implementation
return AudioClip.Create("TTS_Audio", audioData.Length / 2, 1, 22050, false);
}

void Update()
{
#if !UNITY_WEBGL || UNITY_EDITOR
ttsWebSocket?.DispatchMessageQueue();
#endif
}
}

[System.Serializable]
public class TTSRequest
{
public string text;
public string language;
public int chunk_size;
}

[System.Serializable]
public class TTSResponse
{
public string type;
public string data;
public int chunk_index;
}</code></pre>

---

<h3>ğŸ¤ <strong>Step 3: Real-Time Speech Transcription</strong></h3>

<div style="background: linear-gradient(135deg, #ed8936 0%, #dd6b20 100%); padding: 20px; border-radius: 10px; margin: 10px 0;">

<strong>ğŸ”— Endpoint:</strong> <code>WebSocket /api/v1/stt/stream</code> (Real-time)

<strong>ğŸ¬ Process Flow:</strong>
<ul><li>âœ… Greeting finishes â†’ start listening to customer</li>
<li>ğŸ¤ Stream microphone audio in chunks to STT</li>
<li>ğŸ“ Receive partial transcriptions as customer speaks</li>
<li>ğŸ Get final transcription when customer stops â†’ triggers Step 4</li>

</ul></div>

<strong>Usage (WebSocket):</strong>
<pre><code class="json">{
"audio_data": "base64_audio_chunk",
"chunk_index": 1,
"is_final": false,
"language": "ar",
"sample_rate": 16000
}</code></pre>

<strong>Response (Final):</strong>
<pre><code class="json">{
"type": "final",
"text": "Ø£Ø±ÙŠØ¯ Ø´Ø±Ø§Ø¡ Ù‚Ù…ÙŠØµ Ø£Ø²Ø±Ù‚ Ù„Ù„Ø¹Ù…Ù„",
"chunk_index": 3,
"confidence": 0.94,
"is_final": true
}</code></pre>

<h4>ğŸ§ª <strong>Postman Testing Example</strong></h4>

<details>
<summary><b>ğŸ“® STT WebSocket Testing (Advanced)</b></summary>

<strong>âš ï¸ Note:</strong> Postman WebSocket support is limited. For full testing, use a WebSocket client or the Unity implementation.

<strong>ğŸ”§ Alternative: Test STT Batch Endpoint:</strong>
<ul><li><strong>Method:</strong> <code>POST</code></li>
<li><strong>URL:</strong> <code>http://localhost:8000/api/v1/stt/transcribe</code></li>
<li><strong>Headers:</strong></li>
</ul>- <code>Content-Type</code>: <code>multipart/form-data</code> (auto-set)
<ul><li><strong>Body â†’ form-data:</strong></li>
</ul>- Key: <code>file</code> | Type: <code>File</code> | Value: Upload a <code>.wav</code> or <code>.mp3</code> audio file
- Key: <code>language</code> | Type: <code>Text</code> | Value: <code>ar</code>

<strong>ğŸ“¤ Expected Response:</strong>
<pre><code class="json">{
"transcription": "Ø£Ø±ÙŠØ¯ Ø´Ø±Ø§Ø¡ Ù‚Ù…ÙŠØµ Ø£Ø²Ø±Ù‚ Ù„Ù„Ø¹Ù…Ù„",
"language": "ar",
"confidence": 0.94,
"processing_time": 2.1,
"audio_duration": 3.5
}</code></pre>

<strong>ğŸ”§ WebSocket Testing (Using WebSocket Client):</strong>
<ul><li><strong>URL:</strong> <code>ws://localhost:8000/api/v1/stt/stream</code></li>
<li><strong>Send Message (Audio Chunk):</strong></li>
</ul><pre><code class="json">{
"audio_data": "UklGRiQAAABXQVZFZm10IBAAAAABAAEA...",
"chunk_index": 1,
"is_final": false,
"language": "ar",
"sample_rate": 16000
}</code></pre>
<ul><li><strong>Send Final Message:</strong></li>
</ul><pre><code class="json">{
"audio_data": "",
"chunk_index": 2,
"is_final": true,
"language": "ar",
"sample_rate": 16000
}</code></pre>
<ul><li><strong>Receive:</strong> Partial and final transcription responses</li>

</ul><strong>ğŸ¤ Audio Requirements:</strong>
<ul><li>Format: WAV, MP3, or base64 encoded</li>
<li>Sample Rate: 16000 Hz recommended</li>
<li>Language: Arabic (<code>ar</code>) for this flow</li>

</ul></details>

<strong>Unity Example:</strong>
<pre><code class="csharp">using UnityEngine;
using NativeWebSocket;
using System.Collections;

public class STTManager : MonoBehaviour
{
public string serverUrl = "http://localhost:8000";
public AudioSource microphoneSource;

private WebSocket sttWebSocket;
private AudioClip microphoneClip;
private bool isListening = false;
private int chunkIndex = 0;
private string microphoneDevice;

async void Start()
{
// Initialize microphone
if (Microphone.devices.Length > 0)
{
microphoneDevice = Microphone.devices[0];
}

// Initialize STT WebSocket
string wsUrl = serverUrl.Replace("http://", "ws://") + "/api/v1/stt/stream";
sttWebSocket = new WebSocket(wsUrl);

sttWebSocket.OnOpen += () => Debug.Log("STT WebSocket connected");
sttWebSocket.OnMessage += HandleSTTMessage;

await sttWebSocket.Connect();
}

public void StartListening()
{
if (!isListening && !string.IsNullOrEmpty(microphoneDevice))
{
isListening = true;
chunkIndex = 0;

// Start recording
microphoneClip = Microphone.Start(microphoneDevice, true, 10, 16000);

// Start sending audio chunks
StartCoroutine(SendAudioChunks());
}
}

private IEnumerator SendAudioChunks()
{
while (isListening)
{
yield return new WaitForSeconds(1.0f);

if (sttWebSocket.State == WebSocketState.Open)
{
SendCurrentAudioChunk(false);
}
}
}

private async void SendCurrentAudioChunk(bool isFinal)
{
if (microphoneClip == null) return;

// Get audio data
float[] audioData = new float[16000]; // 1 second at 16kHz
microphoneClip.GetData(audioData, 0);

// Convert to bytes and base64
byte[] audioBytes = ConvertFloatArrayToBytes(audioData);
string base64Audio = System.Convert.ToBase64String(audioBytes);

var request = new STTRequest
{
audio_data = base64Audio,
chunk_index = chunkIndex++,
is_final = isFinal,
language = "ar",
sample_rate = 16000
};

string json = JsonUtility.ToJson(request);
await sttWebSocket.SendText(json);
}

private void HandleSTTMessage(byte[] bytes)
{
string message = System.Text.Encoding.UTF8.GetString(bytes);
var response = JsonUtility.FromJson<STTResponse>(message);

switch (response.type)
{
case "partial":
Debug.Log($"Partial: {response.text}");
break;
case "final":
Debug.Log($"Final: {response.text}");
isListening = false;
Microphone.End(microphoneDevice);

// Move to Step 4
GetComponent<PersonStatusManager>().AnalyzePersonStatus(response.text);
break;
}
}

private byte[] ConvertFloatArrayToBytes(float[] floatArray)
{
byte[] byteArray = new byte[floatArray.Length * 2];
for (int i = 0; i < floatArray.Length; i++)
{
short sample = (short)(floatArray[i] * 32767f);
byte[] sampleBytes = System.BitConverter.GetBytes(sample);
byteArray[i * 2] = sampleBytes[0];
byteArray[i * 2 + 1] = sampleBytes[1];
}
return byteArray;
}

void Update()
{
#if !UNITY_WEBGL || UNITY_EDITOR
sttWebSocket?.DispatchMessageQueue();
#endif
}
}

[System.Serializable]
public class STTRequest
{
public string audio_data;
public int chunk_index;
public bool is_final;
public string language;
public int sample_rate;
}

[System.Serializable]
public class STTResponse
{
public string type;
public string text;
public int chunk_index;
public float confidence;
public bool is_final;
}</code></pre>

---

<h3>ğŸ‘ï¸ <strong>Step 4: Get Person Status from VLM</strong></h3>

<div style="background: linear-gradient(135deg, #e53e3e 0%, #c53030 100%); padding: 20px; border-radius: 10px; margin: 10px 0;">

<strong>ğŸ”— Endpoint:</strong> <code>POST /api/v1/status/analyze</code> or <code>POST /api/v1/status/capture</code>

<strong>ğŸ¬ Process Flow:</strong>
<ul><li>ğŸ Final transcription received â†’ capture current camera frame</li>
<li>ğŸ“¤ Send frame to VLM Gemma3 for person status analysis</li>
<li>ğŸ“Š Get person demographics, emotions, engagement level</li>
<li>ğŸ”— Combine transcription + status â†’ send to LLM in Step 5</li>

</ul></div>

<strong>Usage:</strong>
<pre><code class="http">POST /api/v1/status/analyze
Content-Type: multipart/form-data

file: [current_frame.jpg]
include_demographics: true
include_emotions: true
include_engagement: true</code></pre>

<strong>Response:</strong>
<pre><code class="json">{
"person_id": "person_123",
"demographics": {
"age_range": "25-35",
"gender": "female",
"appearance": {"clothing_style": "casual"}
},
"emotions": {
"primary_emotion": "interested",
"emotion_confidence": 0.88
},
"engagement": {
"attention_level": 0.92,
"interest_score": 0.85
}
}</code></pre>

<h4>ğŸ§ª <strong>Postman Testing Example</strong></h4>

<details>
<summary><b>ğŸ“® Person Status Analysis Testing</b></summary>

<strong>ğŸ”§ Test Person Status Analysis:</strong>
<ul><li><strong>Method:</strong> <code>POST</code></li>
<li><strong>URL:</strong> <code>http://localhost:8000/api/v1/status/analyze</code></li>
<li><strong>Headers:</strong></li>
</ul>- <code>Content-Type</code>: <code>multipart/form-data</code> (auto-set)
<ul><li><strong>Body â†’ form-data:</strong></li>
</ul>- Key: <code>file</code> | Type: <code>File</code> | Value: Upload a <code>.jpg</code> or <code>.png</code> image with a person
- Key: <code>include_demographics</code> | Type: <code>Text</code> | Value: <code>true</code>
- Key: <code>include_emotions</code> | Type: <code>Text</code> | Value: <code>true</code>
- Key: <code>include_engagement</code> | Type: <code>Text</code> | Value: <code>true</code>
- Key: <code>confidence_threshold</code> | Type: <code>Text</code> | Value: <code>0.5</code>
- Key: <code>analysis_depth</code> | Type: <code>Text</code> | Value: <code>detailed</code>

<strong>ğŸ“¤ Expected Response:</strong>
<pre><code class="json">{
"person_id": "person_abc123",
"demographics": {
"age_range": "25-35",
"gender": "female",
"appearance": {
"clothing_style": "casual",
"dominant_colors": ["blue", "white"]
}
},
"emotions": {
"primary_emotion": "interested",
"emotion_confidence": 0.88,
"secondary_emotions": ["happy", "curious"]
},
"engagement": {
"attention_level": 0.92,
"interest_score": 0.85,
"engagement_duration": 5.2
},
"analysis_metadata": {
"processing_time": 1.2,
"model_version": "gemma3-vlm",
"confidence_score": 0.89
}
}</code></pre>

<strong>ğŸ”§ Alternative: Test Camera Capture Endpoint:</strong>
<ul><li><strong>Method:</strong> <code>POST</code></li>
<li><strong>URL:</strong> <code>http://localhost:8000/api/v1/status/capture</code></li>
<li><strong>Body â†’ raw (JSON):</strong></li>
</ul><pre><code class="json">{
"camera_id": "default",
"resolution": "640x480",
"include_analysis": true
}</code></pre>

<strong>ğŸ“¤ Expected Response:</strong>
<pre><code class="json">{
"image_url": "/uploads/captured_frame_12345.jpg",
"analysis": {
"person_detected": true,
"person_count": 1,
"primary_emotion": "interested"
},
"capture_timestamp": "2024-01-15T10:30:00Z"
}</code></pre>

<strong>ğŸ–¼ï¸ Image Requirements:</strong>
<ul><li>Format: JPG, PNG supported</li>
<li>Resolution: 640x480 minimum recommended</li>
<li>Content: Clear view of person's face and upper body</li>
<li>Lighting: Well-lit scene for better analysis</li>

</ul></details>

<strong>Unity Example:</strong>
<pre><code class="csharp">using UnityEngine;
using UnityEngine.Networking;
using System.Collections;

public class PersonStatusManager : MonoBehaviour
{
public string serverUrl = "http://localhost:8000";
public Camera statusCamera;

public void AnalyzePersonStatus(string transcription)
{
StartCoroutine(GetPersonStatusAndSendToChat(transcription));
}

private IEnumerator GetPersonStatusAndSendToChat(string transcription)
{
// Step 1: Capture current frame from camera
byte[] imageData = CaptureImageFromCamera();

// Step 2: Send to VLM for analysis
WWWForm form = new WWWForm();
form.AddBinaryData("file", imageData, "current_frame.jpg", "image/jpeg");
form.AddField("include_demographics", "true");
form.AddField("include_emotions", "true");
form.AddField("include_engagement", "true");

using (UnityWebRequest request = UnityWebRequest.Post($"{serverUrl}/api/v1/status/analyze", form))
{
yield return request.SendWebRequest();

if (request.result == UnityWebRequest.Result.Success)
{
var personStatus = JsonUtility.FromJson<PersonStatus>(request.downloadHandler.text);
Debug.Log($"Person status: {personStatus.emotions.primary_emotion}");

// Step 3: Send transcription + status to LLM
GetComponent<ChatManager>().SendContextualMessage(transcription, personStatus);
}
}
}

private byte[] CaptureImageFromCamera()
{
// Capture current frame from camera
RenderTexture renderTexture = new RenderTexture(640, 480, 24);
statusCamera.targetTexture = renderTexture;
statusCamera.Render();

RenderTexture.active = renderTexture;
Texture2D texture = new Texture2D(640, 480, TextureFormat.RGB24, false);
texture.ReadPixels(new Rect(0, 0, 640, 480), 0, 0);
texture.Apply();

byte[] imageData = texture.EncodeToJPG(85);

// Cleanup
statusCamera.targetTexture = null;
RenderTexture.active = null;
DestroyImmediate(renderTexture);
DestroyImmediate(texture);

return imageData;
}
}

[System.Serializable]
public class PersonStatus
{
public string person_id;
public Demographics demographics;
public Emotions emotions;
public Engagement engagement;
}

[System.Serializable]
public class Demographics
{
public string age_range;
public string gender;
public Appearance appearance;
}

[System.Serializable]
public class Appearance
{
public string clothing_style;
}

[System.Serializable]
public class Emotions
{
public string primary_emotion;
public float emotion_confidence;
}

[System.Serializable]
public class Engagement
{
public float attention_level;
public float interest_score;
}</code></pre>

---

<h3>ğŸ§  <strong>Step 5: Send to LLM for Context-Aware Response</strong></h3>

<div style="background: linear-gradient(135deg, #9f7aea 0%, #805ad5 100%); padding: 20px; border-radius: 10px; margin: 10px 0;">

<strong>ğŸ”— Endpoint:</strong> <code>POST /api/v1/chat/message</code> or <code>POST /api/v1/chat/stream</code> (SSE)

<strong>ğŸ¬ Process Flow:</strong>
<ul><li>ğŸ”— Combine customer transcription + person status into contextual message</li>
<li>ğŸ“¤ Send to LLM for intelligent, personalized response</li>
<li>ğŸ¯ LLM considers question + demographics + emotions + engagement</li>
<li>âš¡ Stream response in real-time â†’ triggers Step 6 (TTS)</li>

</ul></div>

<strong>Usage:</strong>
<pre><code class="http">POST /api/v1/chat/message
Content-Type: application/json

{
"message": "Customer said: 'Ø£Ø±ÙŠØ¯ Ø´Ø±Ø§Ø¡ Ù‚Ù…ÙŠØµ Ø£Ø²Ø±Ù‚ Ù„Ù„Ø¹Ù…Ù„'\n\nCustomer status:\n- Age: 25-35\n- Gender: female\n- Emotion: interested (0.88)\n- Engagement: 0.92 attention\n\nPlease provide a personalized Arabic response.",
"session_id": "sess_123"
}</code></pre>

<strong>Response:</strong>
<pre><code class="json">{
"response": "Ù…Ø±Ø­Ø¨Ø§Ù‹! Ø£Ø±Ù‰ Ø£Ù†Ùƒ Ù…Ù‡ØªÙ…Ø© Ø¨Ù‚Ù…ÙŠØµ Ø£Ø²Ø±Ù‚ Ù„Ù„Ø¹Ù…Ù„. Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø£Ø³Ù„ÙˆØ¨Ùƒ Ø§Ù„Ø¹Ù…Ù„ÙŠØŒ Ø£Ù†ØµØ­Ùƒ Ø¨Ù‚Ù…ÙŠØµ Ù‚Ø·Ù†ÙŠ ÙƒÙ„Ø§Ø³ÙŠÙƒÙŠ Ø£Ø²Ø±Ù‚ ÙØ§ØªØ­ - Ù…Ø±ÙŠØ­ ÙˆØ£Ù†ÙŠÙ‚ Ù„Ù„Ø¹Ù…Ù„. Ù…Ø§ Ø±Ø£ÙŠÙƒØŸ",
"session_id": "sess_123",
"processing_time": 2.1,
"confidence": 0.95
}</code></pre>

<h4>ğŸ§ª <strong>Postman Testing Example</strong></h4>

<details>
<summary><b>ğŸ“® Context-Aware Chat Testing</b></summary>

<strong>ğŸ”§ Test Regular Chat Message:</strong>
<ul><li><strong>Method:</strong> <code>POST</code></li>
<li><strong>URL:</strong> <code>http://localhost:8000/api/v1/chat/message</code></li>
<li><strong>Headers:</strong></li>
</ul>- <code>Content-Type</code>: <code>application/json</code>
<ul><li><strong>Body â†’ raw (JSON):</strong></li>
</ul><pre><code class="json">{
"message": "Customer said: 'Ø£Ø±ÙŠØ¯ Ø´Ø±Ø§Ø¡ Ù‚Ù…ÙŠØµ Ø£Ø²Ø±Ù‚ Ù„Ù„Ø¹Ù…Ù„'\n\nCustomer status:\n- Age: 25-35\n- Gender: female\n- Emotion: interested (0.88)\n- Engagement: 0.92 attention\n\nPlease provide a personalized Arabic response.",
"session_id": "sess_123",
"context": {
"customer_transcription": "Ø£Ø±ÙŠØ¯ Ø´Ø±Ø§Ø¡ Ù‚Ù…ÙŠØµ Ø£Ø²Ø±Ù‚ Ù„Ù„Ø¹Ù…Ù„",
"person_status": {
"age_range": "25-35",
"gender": "female",
"emotion": "interested",
"engagement": 0.92
}
}
}</code></pre>

<strong>ğŸ“¤ Expected Response:</strong>
<pre><code class="json">{
"response": "Ù…Ø±Ø­Ø¨Ø§Ù‹! Ø£Ø±Ù‰ Ø£Ù†Ùƒ Ù…Ù‡ØªÙ…Ø© Ø¨Ù‚Ù…ÙŠØµ Ø£Ø²Ø±Ù‚ Ù„Ù„Ø¹Ù…Ù„. Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø£Ø³Ù„ÙˆØ¨Ùƒ Ø§Ù„Ø¹Ù…Ù„ÙŠØŒ Ø£Ù†ØµØ­Ùƒ Ø¨Ù‚Ù…ÙŠØµ Ù‚Ø·Ù†ÙŠ ÙƒÙ„Ø§Ø³ÙŠÙƒÙŠ Ø£Ø²Ø±Ù‚ ÙØ§ØªØ­ - Ù…Ø±ÙŠØ­ ÙˆØ£Ù†ÙŠÙ‚ Ù„Ù„Ø¹Ù…Ù„. Ù…Ø§ Ø±Ø£ÙŠÙƒØŸ",
"session_id": "sess_123",
"processing_time": 2.1,
"confidence": 0.95,
"context_used": true,
"recommendations": [
{
"product": "Blue Cotton Work Shirt",
"price": "120 SAR",
"confidence": 0.89
}
]
}</code></pre>

<strong>ğŸ”§ Test Streaming Chat (SSE):</strong>
<ul><li><strong>Method:</strong> <code>POST</code></li>
<li><strong>URL:</strong> <code>http://localhost:8000/api/v1/chat/stream</code></li>
<li><strong>Headers:</strong></li>
</ul>- <code>Content-Type</code>: <code>application/json</code>
- <code>Accept</code>: <code>text/event-stream</code>
<ul><li><strong>Body â†’ raw (JSON):</strong></li>
</ul><pre><code class="json">{
"message": "Ø£Ø±ÙŠØ¯ Ø´Ø±Ø§Ø¡ Ø­Ø°Ø§Ø¡ Ø±ÙŠØ§Ø¶ÙŠ Ù„Ù„Ø¬Ø±ÙŠ",
"session_id": "sess_456"
}</code></pre>

<strong>ğŸ“¤ Expected SSE Stream:</strong>
<pre><code class="">data: {"type": "start", "session_id": "sess_456"}

data: {"type": "token", "content": "Ù…Ø±Ø­Ø¨Ø§Ù‹", "session_id": "sess_456"}

data: {"type": "token", "content": "!", "session_id": "sess_456"}

data: {"type": "token", "content": " Ø£Ø±Ù‰", "session_id": "sess_456"}

data: {"type": "complete", "full_response": "Ù…Ø±Ø­Ø¨Ø§Ù‹! Ø£Ø±Ù‰ Ø£Ù†Ùƒ ØªØ¨Ø­Ø« Ø¹Ù† Ø­Ø°Ø§Ø¡ Ø±ÙŠØ§Ø¶ÙŠ Ù„Ù„Ø¬Ø±ÙŠ...", "session_id": "sess_456"}</code></pre>

<strong>ğŸ”§ Simple Chat Test:</strong>
<ul><li><strong>Method:</strong> <code>POST</code></li>
<li><strong>URL:</strong> <code>http://localhost:8000/api/v1/chat/message</code></li>
<li><strong>Body â†’ raw (JSON):</strong></li>
</ul><pre><code class="json">{
"message": "Ù…Ø±Ø­Ø¨Ø§ØŒ ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒØŸ",
"session_id": "test_session"
}</code></pre>

<strong>ğŸ—£ï¸ Message Requirements:</strong>
<ul><li>Language: Arabic or English supported</li>
<li>Context: Include person status for better responses</li>
<li>Session: Use consistent session_id for conversation flow</li>

</ul></details>

<strong>Unity Example:</strong>
<pre><code class="csharp">using UnityEngine;
using UnityEngine.Networking;
using System.Collections;

public class ChatManager : MonoBehaviour
{
public string serverUrl = "http://localhost:8000";

public void SendContextualMessage(string transcription, PersonStatus personStatus)
{
StartCoroutine(GetLLMResponse(transcription, personStatus));
}

private IEnumerator GetLLMResponse(string transcription, PersonStatus status)
{
// Create contextual message combining transcription + person status
string contextualMessage = CreateContextualMessage(transcription, status);

var chatRequest = new ChatRequest
{
message = contextualMessage,
session_id = "sess_" + System.DateTime.Now.Ticks
};

string json = JsonUtility.ToJson(chatRequest);

using (UnityWebRequest request = new UnityWebRequest($"{serverUrl}/api/v1/chat/message", "POST"))
{
byte[] bodyRaw = System.Text.Encoding.UTF8.GetBytes(json);
request.uploadHandler = new UploadHandlerRaw(bodyRaw);
request.downloadHandler = new DownloadHandlerBuffer();
request.SetRequestHeader("Content-Type", "application/json");

yield return request.SendWebRequest();

if (request.result == UnityWebRequest.Result.Success)
{
var response = JsonUtility.FromJson<ChatResponse>(request.downloadHandler.text);
Debug.Log($"LLM Response: {response.response}");

// Move to Step 6: Stream response to TTS
GetComponent<TTSManager>().SpeakResponse(response.response);
}
}
}

private string CreateContextualMessage(string transcription, PersonStatus status)
{
return $@"Customer said: ""{transcription}""

Customer status:
<ul><li>Age: {status.demographics.age_range}</li>
<li>Gender: {status.demographics.gender}</li>
<li>Style: {status.demographics.appearance.clothing_style}</li>
<li>Emotion: {status.emotions.primary_emotion} ({status.emotions.emotion_confidence:F2})</li>
<li>Engagement: {status.engagement.attention_level:F2} attention, {status.engagement.interest_score:F2} interest</li>

</ul>Please provide a personalized response in Arabic based on their question and current status.";
}
}

[System.Serializable]
public class ChatRequest
{
public string message;
public string session_id;
}

[System.Serializable]
public class ChatResponse
{
public string response;
public string session_id;
public float processing_time;
public float confidence;
}</code></pre>

---

<h3>ğŸ”Š <strong>Step 6: Stream Response to Real-Time TTS</strong></h3>

<div style="background: linear-gradient(135deg, #3182ce 0%, #2c5282 100%); padding: 20px; border-radius: 10px; margin: 10px 0;">

<strong>ğŸ”— Endpoint:</strong> <code>WebSocket /api/v1/tts/stream</code> (Real-time)

<strong>ğŸ¬ Process Flow:</strong>
<ul><li>ğŸ§  LLM response received â†’ immediately send to TTS</li>
<li>ğŸ“¤ Stream Arabic response text to TTS WebSocket</li>
<li>ğŸµ Receive audio chunks and play in real-time</li>
<li>ğŸ¯ Customer hears intelligent response â†’ ready for next interaction</li>

</ul></div>

<strong>Usage (WebSocket):</strong>
<pre><code class="json">{
"text": "Ù…Ø±Ø­Ø¨Ø§Ù‹! Ø£Ø±Ù‰ Ø£Ù†Ùƒ Ù…Ù‡ØªÙ…Ø© Ø¨Ù‚Ù…ÙŠØµ Ø£Ø²Ø±Ù‚ Ù„Ù„Ø¹Ù…Ù„. Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø£Ø³Ù„ÙˆØ¨Ùƒ Ø§Ù„Ø¹Ù…Ù„ÙŠØŒ Ø£Ù†ØµØ­Ùƒ Ø¨Ù‚Ù…ÙŠØµ Ù‚Ø·Ù†ÙŠ ÙƒÙ„Ø§Ø³ÙŠÙƒÙŠ Ø£Ø²Ø±Ù‚ ÙØ§ØªØ­ - Ù…Ø±ÙŠØ­ ÙˆØ£Ù†ÙŠÙ‚ Ù„Ù„Ø¹Ù…Ù„. Ù…Ø§ Ø±Ø£ÙŠÙƒØŸ",
"language": "ar",
"chunk_size": 1024
}</code></pre>

<strong>Response (Streaming):</strong>
<pre><code class="json">{
"type": "audio_chunk",
"data": "base64_audio_data",
"chunk_index": 1
}</code></pre>

<strong>Unity Example:</strong>
<pre><code class="csharp">// Add this method to the TTSManager class from Step 2

public async void SpeakResponse(string responseText)
{
if (ttsWebSocket.State == WebSocketState.Open)
{
var request = new TTSRequest
{
text = responseText,
language = "ar",
chunk_size = 1024
};

string json = JsonUtility.ToJson(request);
await ttsWebSocket.SendText(json);

Debug.Log($"Streaming response to TTS: {responseText.Substring(0, 50)}...");
}
}

// Update the HandleTTSMessage method to handle response completion
private void HandleTTSMessage(byte[] bytes)
{
string message = System.Text.Encoding.UTF8.GetString(bytes);
var response = JsonUtility.FromJson<TTSResponse>(message);

switch (response.type)
{
case "audio_chunk":
PlayAudioChunk(response.data);
break;
case "complete":
if (response.data.Contains("Ø§Ù‡Ù„Ø§ Ø¨ÙŠÙƒ ØªØ¹Ø§Ù„ÙŠ"))
{
// Greeting complete - start listening
Debug.Log("Greeting complete - starting to listen");
GetComponent<STTManager>().StartListening();
}
else
{
// Response complete - ready for next customer
Debug.Log("Response complete - ready for next customer");
ResetForNextCustomer();
}
break;
}
}

private void ResetForNextCustomer()
{
// Reset system for next customer interaction
Debug.Log("System reset - monitoring for next customer");
GetComponent<PersonTracker>().StartCoroutine(GetComponent<PersonTracker>().DetectPerson());
}</code></pre>

---

<h2>ğŸ§ª <strong>Complete Postman Testing Collection</strong></h2>

<h3><strong>ğŸ“¦ Postman Collection Summary</strong></h3>

<div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 20px; border-radius: 10px; margin: 10px 0;">

<strong>ğŸ”§ Essential Testing Endpoints:</strong>

| <strong>Step</strong> | <strong>Endpoint</strong> | <strong>Method</strong> | <strong>Purpose</strong> | <strong>Test File</strong> |
|----------|--------------|------------|-------------|---------------|
| 1 | <code>/api/v1/tracking/upload</code> | POST | Upload video for person detection | Video file (MP4) |
| 1 | <code>/api/v1/tracking/process/{id}</code> | POST | Process uploaded video | Session ID from upload |
| 2 | <code>/api/v1/tts/synthesize</code> | POST | Generate greeting audio | Arabic text |
| 3 | <code>/api/v1/stt/transcribe</code> | POST | Transcribe customer speech | Audio file (WAV/MP3) |
| 4 | <code>/api/v1/status/analyze</code> | POST | Analyze person status | Image file (JPG/PNG) |
| 5 | <code>/api/v1/chat/message</code> | POST | Get LLM response | JSON message |
| 6 | <code>/api/v1/tts/synthesize</code> | POST | Generate response audio | Arabic response text |

</div>

<h3><strong>ğŸš€ Quick Test Sequence</strong></h3>

<details>
<summary><b>ğŸ“‹ Step-by-Step Testing Guide</b></summary>

<strong>ğŸ¯ Complete Flow Test (5 minutes):</strong>

<ul><li><strong>ğŸ¥ Test Person Detection:</strong></li>
</ul><pre><code class="bash">   POST http://localhost:8000/api/v1/tracking/upload
# Upload: sample_video.mp4, confidence_threshold: 0.5
# Expected: session_id returned
<code></code>`

<ul><li><strong>ğŸµ Test Greeting Audio:</strong></li>
</ul><code></code>`bash
POST http://localhost:8000/api/v1/tts/synthesize
# Body: {"text": "Ø§Ù‡Ù„Ø§ Ø¨ÙŠÙƒ ØªØ¹Ø§Ù„ÙŠ", "language": "ar"}
# Expected: audio_url returned, test playback
<code></code>`

<ul><li><strong>ğŸ¤ Test Speech Recognition:</strong></li>
</ul><code></code>`bash
POST http://localhost:8000/api/v1/stt/transcribe
# Upload: arabic_speech.wav, language: ar
# Expected: Arabic transcription returned
<code></code>`

<ul><li><strong>ğŸ‘ï¸ Test Person Analysis:</strong></li>
</ul><code></code>`bash
POST http://localhost:8000/api/v1/status/analyze
# Upload: person_image.jpg, include_demographics: true
# Expected: person status with emotions/demographics
<code></code>`

<ul><li><strong>ğŸ§  Test Context-Aware Chat:</strong></li>
</ul><code></code>`bash
POST http://localhost:8000/api/v1/chat/message
# Body: Combined transcription + person status
# Expected: Personalized Arabic response
<code></code>`

<ul><li><strong>ğŸ”Š Test Response Audio:</strong></li>
</ul><code></code>`bash
POST http://localhost:8000/api/v1/tts/synthesize
# Body: {"text": "[LLM Response]", "language": "ar"}
# Expected: Response audio generated
<code></code>`

</details>

<h3><strong>ğŸ“ Required Test Files</strong></h3>

<div style="background: linear-gradient(135deg, #f7fafc 0%, #edf2f7 100%); padding: 20px; border-radius: 10px; margin: 10px 0; border-left: 4px solid #3182ce;">

<strong>ğŸ¬ Video Files:</strong>
<ul><li><code>sample_video.mp4</code> - Video with person for tracking (3-10 seconds)</li>
<li>Resolution: 640x480 minimum</li>

</ul><strong>ğŸ¤ Audio Files:</strong>
<ul><li><code>arabic_speech.wav</code> - Arabic speech sample for STT</li>
<li>Format: WAV, 16kHz sample rate preferred</li>
<li>Duration: 3-5 seconds of clear Arabic speech</li>

</ul><strong>ğŸ–¼ï¸ Image Files:</strong>
<ul><li><code>person_image.jpg</code> - Clear image of person's face/upper body</li>
<li>Resolution: 640x480 minimum</li>
<li>Good lighting for better analysis</li>

</ul><strong>ğŸ“ Sample Data:</strong>
<ul><li>Arabic text samples for TTS testing</li>
<li>Context-aware message templates</li>
<li>Session IDs for flow continuity</li>

</ul></div>

<h3><strong>âš ï¸ WebSocket Testing Notes</strong></h3>

<div style="background: linear-gradient(135deg, #fed7d7 0%, #feb2b2 100%); padding: 20px; border-radius: 10px; margin: 10px 0; border-left: 4px solid #e53e3e;">

<strong>ğŸ”Œ WebSocket Limitations in Postman:</strong>
<ul><li>Postman has limited WebSocket support</li>
<li>Use alternative tools for real-time testing:</li>
</ul>- <strong>WebSocket King</strong> (Browser extension)
- <strong>Insomnia</strong> (Full WebSocket support)
- <strong>Unity Implementation</strong> (Recommended)

<strong>ğŸ”— WebSocket URLs:</strong>
<ul><li>STT: <code>ws://localhost:8000/api/v1/stt/stream</code></li>
<li>TTS: <code>ws://localhost:8000/api/v1/tts/stream</code></li>

</ul></div>

<h3><strong>âœ… Expected Response Codes</strong></h3>

| <strong>Scenario</strong> | <strong>HTTP Code</strong> | <strong>Description</strong> |
|--------------|---------------|-----------------|
| Successful upload | <code>200 OK</code> | File uploaded and processed |
| Successful analysis | <code>200 OK</code> | Analysis completed |
| Invalid file format | <code>400 Bad Request</code> | Unsupported file type |
| Missing file | <code>422 Unprocessable Entity</code> | Required file not provided |
| Server processing error | <code>500 Internal Server Error</code> | AI model processing failed |
| Model not loaded | <code>503 Service Unavailable</code> | AI models not ready |

---

<h2>ğŸ”„ <strong>Complete Flow Summary</strong></h2>

<h3><strong>ğŸš€ Real-time Customer Interaction Pipeline</strong></h3>

<div style="background: linear-gradient(135deg, #38a169 0%, #2f855a 100%); padding: 20px; border-radius: 10px; margin: 10px 0;">

<strong>ğŸ“Š End-to-End Process:</strong>

<ul><li><strong>ğŸ“¹ Camera Stream</strong> â†’ <code>POST /api/v1/tracking/upload</code> â†’ Person detected</li>
<li><strong>ğŸµ Greeting</strong> â†’ <code>WebSocket /api/v1/tts/stream</code> â†’ "Ø§Ù‡Ù„Ø§ Ø¨ÙŠÙƒ ØªØ¹Ø§Ù„ÙŠ" </li>
<li><strong>ğŸ¤ Customer Speaks</strong> â†’ <code>WebSocket /api/v1/stt/stream</code> â†’ Real-time transcription</li>
<li><strong>ğŸ‘ï¸ Person Status</strong> â†’ <code>POST /api/v1/status/analyze</code> â†’ VLM analysis  </li>
<li><strong>ğŸ§  Intelligent Response</strong> â†’ <code>POST /api/v1/chat/message</code> â†’ Context-aware LLM</li>
<li><strong>ğŸ”Š Audio Response</strong> â†’ <code>WebSocket /api/v1/tts/stream</code> â†’ Customer hears response</li>

</ul></div>

<h3><strong>ğŸ¯ Key Endpoints Reference</strong></h3>

| Step | Endpoint | Type | Purpose |
|------|----------|------|---------|
| 1 | <code>POST /api/v1/tracking/upload</code> | REST | Upload video for person detection |
| 1 | <code>POST /api/v1/tracking/process/{id}</code> | REST | Process video for tracking |
| 2,6 | <code>WebSocket /api/v1/tts/stream</code> | WebSocket | Real-time text-to-speech |
| 3 | <code>WebSocket /api/v1/stt/stream</code> | WebSocket | Real-time speech-to-text |
| 4 | <code>POST /api/v1/status/analyze</code> | REST | VLM person status analysis |
| 5 | <code>POST /api/v1/chat/message</code> | REST | Context-aware LLM response |

<h3><strong>âš¡ Performance Metrics</strong></h3>

<ul><li><strong>ğŸ¯ Detection Latency</strong>: < 100ms person detection</li>
<li><strong>ğŸµ Greeting Response</strong>: < 500ms audio generation  </li>
<li><strong>ğŸ¤ STT Processing</strong>: < 1s partial, < 2s final transcription</li>
<li><strong>ğŸ‘ï¸ Status Analysis</strong>: < 800ms VLM processing</li>
<li><strong>ğŸ§  LLM Response</strong>: < 3s context-aware generation</li>
<li><strong>ğŸ”Š TTS Streaming</strong>: < 500ms first audio chunk</li>
<li><strong>â±ï¸ Total Interaction</strong>: < 7s end-to-end latency</li>

</ul><strong>ğŸ‰ Result: Complete real-time AI customer interaction system!</strong>

<h2>ğŸš€ <strong>Unity Integration Guide</strong></h2>

<h3><strong>ğŸ“¦ Complete Implementation Available</strong></h3>

A comprehensive Unity integration example is available in <code>Unity_Complete_Integration_Example.cs</code> which demonstrates all 6 steps with proper WebSocket connections, microphone/camera integration, and complete flow orchestration.

<h3><strong>âš™ï¸ Unity Setup Requirements</strong></h3>

<div style="background: linear-gradient(135deg, #f7fafc 0%, #edf2f7 100%); padding: 20px; border-radius: 10px; margin: 10px 0; border-left: 4px solid #38a169;">

<strong>ğŸ“‹ Prerequisites:</strong>
<ul><li><strong>Unity 2021.3</strong> or later for full compatibility</li>
<li><strong>NativeWebSocket package</strong> for WebSocket support</li>
<li><strong>Microphone permissions</strong> for audio capture</li>
<li><strong>Camera permissions</strong> for video capture and person analysis</li>

</ul><strong>ğŸ”§ Installation Steps:</strong>
<ul><li>Install NativeWebSocket from Package Manager</li>
<li>Configure microphone permissions in Player Settings</li>
<li>Set up camera permissions for video capture</li>
<li>Import the provided Unity scripts</li>
<li>Configure server URL in inspector</li>
<li>Assign camera and audio source components</li>

</ul><strong>ğŸ¯ Component Setup:</strong>
<ul><li><code>PersonTracker</code> â†’ Attach to GameObject with Camera</li>
<li><code>TTSManager</code> â†’ Attach to GameObject with AudioSource  </li>
<li><code>STTManager</code> â†’ Attach to GameObject with microphone access</li>
<li><code>PersonStatusManager</code> â†’ Attach to GameObject with Camera</li>
<li><code>ChatManager</code> â†’ Attach to main controller GameObject</li>

</ul></div>

<h3><strong>ğŸ—ï¸ Architecture Overview</strong></h3>

The Unity implementation follows a modular architecture where each component handles one step of the interaction flow:
</code></pre>
ğŸ® Unity GameObject Hierarchy:
â”œâ”€â”€ ğŸ“¹ CameraController (PersonTracker + PersonStatusManager)
â”œâ”€â”€ ğŸµ AudioController (TTSManager + STTManager)
â”œâ”€â”€ ğŸ§  AIController (ChatManager)
â””â”€â”€ ğŸ¯ MainController (Flow orchestration)
<code></code>`

<h3><strong>ğŸ”— Integration Flow</strong></h3>

Each Unity script seamlessly connects to the next step in the pipeline, creating a fully automated customer interaction system that requires minimal manual intervention.

<strong>âœ… All endpoints are implemented and ready for seamless Unity integration!</strong> ğŸ‰

---

<div align="center">

<h3><strong>ğŸ“ Support & Documentation</strong></h3>

For technical support or additional integration questions, refer to the complete API documentation and Unity implementation examples provided in this guide.

<strong>ğŸ¯ Happy Coding!</strong>

</div>
</body>
</html>
